{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f688d87-01cd-4ce0-9ece-166ab3b9b3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e38e591230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af1dbc7-57bd-4d3f-8230-09e2b30cdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('harrypotterbooks.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c7d84b-dc43-4c0a-af3b-57da2df9e51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BOY WHO LIVED Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much .They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense .Mr Dursley was the director of a firm called Grunnings which made drills .He was a big beefy man with hardly any neck although he did have a very large mustache .Mrs Dursley was thin and blonde and had nearly twice the usual amo\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff9c072-b7d6-4356-96c7-5d3f6fe0c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz~‘•■□\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size) # veamos el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4944890-0921-4336-a1b3-e7302c214bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 48, 0, 21, 40, 57, 57, 64]\n",
      "Hi Harry\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars)} # token:id\n",
    "itos = { i:ch for i,ch in enumerate(chars)} # id:token\n",
    "encode = lambda s: [stoi[c] for c in s] # devuelve lista de ids en base a tokens proporcionados\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # devuelve el texto en base a los ids proporcionados\n",
    "print(encode('Hi Harry'))\n",
    "print(decode(encode('Hi Harry')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9dfa437-5d8a-45d3-be70-13e06f03c83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 21, 18,  0, 15, 28, 38,  0, 36, 21, 28,  0, 25, 22, 35, 18, 17,  0,\n",
      "        26, 57,  0, 40, 53, 43,  0, 26, 57, 58,  0, 17, 60, 57, 58, 51, 44, 64,\n",
      "         0, 54, 45,  0, 53, 60, 52, 41, 44, 57,  0, 45, 54, 60, 57,  0, 29, 57,\n",
      "        48, 61, 44, 59,  0, 17, 57, 48, 61, 44,  0, 62, 44, 57, 44,  0, 55, 57,\n",
      "        54, 60, 43,  0, 59, 54,  0, 58, 40, 64,  0, 59, 47, 40, 59,  0, 59, 47,\n",
      "        44, 64,  0, 62, 44, 57, 44,  0, 55, 44])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long) # transformamos el libro en ids\n",
    "print(data[:100]) # primeros 100 carácteres del libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16af16b4-9272-496d-8b7f-4e590132f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # ocuparemos el 90% del libro para entrenar\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] # 10% de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ed9c3d-9c9f-4168-80dd-b8cbc0d44dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # tomamos ids del texto según el tamaño del batch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # extraemos parrafos según la ventana de contexto\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # desplazamos una posición respecto a x\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train') # (B, T)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size): # para cada token\n",
    "        context = xb[b, :t+1] # ventana contexto\n",
    "        target = yb[b,t]\n",
    "        #print(f'Cuando el input es {context.tolist()}, el target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc066307-73b1-4e06-983e-1426a52b2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # no almacenamos variables intermedias\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out # retorna diccionario con la media de losses para train y val según las iteraciones que definamos en eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0829790-3324-44d3-9eb9-90e4d986ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        # el buffer es como un tensor, pero no se modifica durante el entrenamiento\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # agregamos escalamiento 1/sqrt(C)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52fec77-47d5-426d-b81c-451443adf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Multiples cabezales en paralelo\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) # concatenamos en la última dimensióm\n",
    "        out = self.proj(out) # proyección del resultado\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "        # cada cabezal tiene como salida (B, T, n_embd/num_heads), después concatenamos en un solo vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33dd80af-f3a2-416b-8528-85df88626319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # proyección \n",
    "            # multiplicamos por 4, ya que en el paper \"All Atenttion is You Need\" ocupa 4 veces la dimensión de embeddings\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e23440-9863-4dd4-9609-8b9c7aa95d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # agregamos conexión residual para que no se pierda información\n",
    "        # a diferencia del paper original, el batch norm se está aplicando antes del feed forward\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e25b0fe7-c89e-474d-a75b-4edede2eb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construimos un decoder, cómo es un modelo de generación de texto no es necesario ocupar el encoder\n",
    "class PotterGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        # son varios bloques de multihead y feedforward\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            idx_cond = idx[:, -block_size:] # recortamos el contexto en caso de que idx supere el block_size\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # último token o embedding de cada fila (mantiene dimensiones B y C), (B, C)\n",
    "            probs = F.softmax(logits, dim = 1) # softmax en la última dimensión (vocab_size)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # obtenemos id desde distribución multinomial\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42d039f-e0f8-4d47-9ac8-d30d280e0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametros\n",
    "batch_size = 32\n",
    "block_size = 50 # ventana de contexto utilizada en la función get_batch()\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 200 # iteraciones ocupadas en la validación\n",
    "n_embd = 64 # cada head tiene 64/4 dimensiones\n",
    "n_head = 4 # 4 cabezales\n",
    "n_layer = 3 # número de blocks\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbcd175c-138a-429c-aea1-9e9b2a04bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PotterGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38d48a31-31b6-406e-bcd6-f05cdb987b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e7cdffc-781b-4832-89bd-43c27e29a2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4685, val loss 4.4683\n",
      "step 500: train loss 2.6340, val loss 2.6161\n",
      "step 1000: train loss 2.4822, val loss 2.4601\n",
      "step 1500: train loss 2.4179, val loss 2.3975\n",
      "step 2000: train loss 2.3762, val loss 2.3575\n",
      "step 2500: train loss 2.3424, val loss 2.3239\n",
      "step 3000: train loss 2.3200, val loss 2.3012\n",
      "step 3500: train loss 2.2871, val loss 2.2638\n",
      "step 4000: train loss 2.2556, val loss 2.2362\n",
      "step 4500: train loss 2.2267, val loss 2.1964\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True) \n",
    "    loss.backward() \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7786fd58-4d26-46c3-a8b2-e5ef2d44096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pooiedit ing anig th ce creathulitw pey .Thow brerer wof .Whit yousherd fame arave tinsle wat faryankenst herak to becoverbe he to ack Whanor ionetle hat thar Dein nott id wang wetrces any us itwot he whariow thastelly grehabbe wamed sooord Harry Pind yon sleru faryat las woup wis winde shilroy ed llupald atts Cadd hoitunc and 3rot Wiole to .Ho .Thiry Dange avechn Cryednt .Theoll Hant pland of coferpeds eas and noaGlores Tavedow tho .Gustould thour le e ..Criurt wailfornd latdok dund .Rmuron the roo gvit He waine th ate Fo why higgokenther istom ered latigh touusar lugt ay dli yof cerontsh thillikce homau d warye to thar an ucteund as gendfor hi tt yronxppacielde ogely therle me herte Haw ed hawss cerhe bumepperet thedy meaw garkilos te yast thagrouir andtk theedvo Dithe the rome caghe wass hre .Thor dowerabo eas tint bull that shaved his Proor ownemtes buplitand abrofim gom clonche CacheMrte Ror fre Deromppelld .Cre belesanis duvee .Buint wofor on estere meanrtcurmand iaifded thas evelllende H thaistin hik hily thald th eald on Mable Lrs end shimuons O.ing stop Hel crawho y wet ere thingand grearlis baved y .W .Harcche  hed ?FreampeJayemser evee kund act Hom tugoklssiot y ar loud wend do Bre .I scou Sileis roung wooud antse foted vefloul .Emroup .Rereren muleaved .Harmiorry is inad thears .Harrard ino y grearysers .aike te thie toup t sthe sey samscime saiord stow hert con inimenyer de yavand fara mrinled y wind aned hisaimked inise tom wap acrou thet tharembe pont to ay me dre funot therde hinsecpoor tors by onoley topour .e yof Bcourseld fextfersut houde antede ved Proy veamee thim s alrese tfro as blen meeld smead .Bly meandeith to agho cofe camsilly lages casp HeRory ffleemeera!Sing usstuy tss re youpapf it nandeerin itembe yod ssso at of .Cormy finteed enoing anerfere owom juttestseor H0 palleand Qus penaccoutored it aisidsartse ?M wotly thestiche eaingllat tck hive fond eay qufof gothat thew mon the Bes tuthe blallat Pull angand Hte Harray hedld g the wower a\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype = torch.long) \n",
    "print(decode(model.generate(idx, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23f698-5e41-4fdb-a856-053b1e71ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrené una versión en gpu con batch_size = 64 , block_size = 256 , learning_rate = 3e-4 , n_embd = 384 , n_head = 6 , n_layer = 6\n",
    "model = torch.load('potter_gpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebbafd-8268-4128-9aae-3896fe825609",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(model.generate(idx, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
